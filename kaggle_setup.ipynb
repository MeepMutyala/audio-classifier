{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸµ Audio Classification on Kaggle (FREE GPU!)\n",
        "\n",
        "Train Mamba, Liquid S4, and V-JEPA2 models on ESC-50 dataset using Kaggle's free Tesla P100 GPU.\n",
        "\n",
        "## ğŸš€ Quick Start\n",
        "1. **Enable GPU**: Settings â†’ Accelerator â†’ GPU T4 x2 (or P100)\n",
        "2. **Run all cells** below\n",
        "3. **Choose your model** to train\n",
        "\n",
        "## ğŸ“Š Kaggle Resources\n",
        "- **GPU**: Tesla P100 (16GB memory)\n",
        "- **Storage**: 20GB\n",
        "- **Session**: 9 hours max\n",
        "- **Weekly GPU**: 30 hours (resets Saturday)\n",
        "\n",
        "## ğŸ¯ Your Project Stats\n",
        "- **Dataset**: ESC-50 (~1.3GB)\n",
        "- **Training time**: 2-5 hours per model\n",
        "- **Memory usage**: 8-12GB GPU memory\n",
        "- **Perfect fit**: âœ… All models train comfortably!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability and specs\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"ğŸ” Checking Kaggle environment...\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"ğŸ® GPU: {gpu_name}\")\n",
        "    print(f\"ğŸ’¾ GPU Memory: {gpu_memory:.1f} GB\")\n",
        "    \n",
        "    # Check if it's a good GPU for our project\n",
        "    if gpu_memory >= 12:\n",
        "        print(\"âœ… Perfect! This GPU can handle all our models\")\n",
        "    elif gpu_memory >= 8:\n",
        "        print(\"âœ… Good! This GPU can handle Mamba and Liquid S4\")\n",
        "    else:\n",
        "        print(\"âš ï¸  Limited GPU memory - use smaller batch sizes\")\n",
        "else:\n",
        "    print(\"âŒ No GPU detected! Please enable GPU in Settings â†’ Accelerator\")\n",
        "\n",
        "# Check available storage\n",
        "import shutil\n",
        "total, used, free = shutil.disk_usage(\"/kaggle/working\")\n",
        "print(f\"ğŸ’¿ Available storage: {free // (1024**3)} GB\")\n",
        "\n",
        "print(\"\\nğŸ¯ Ready to start training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (Kaggle has most pre-installed)\n",
        "print(\"ğŸ“¦ Installing additional packages...\")\n",
        "\n",
        "# Install packages not in Kaggle's default environment\n",
        "!pip install einops wandb\n",
        "\n",
        "print(\"âœ… Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone your repository\n",
        "print(\"ğŸ“¥ Cloning repository...\")\n",
        "\n",
        "# Replace with your actual GitHub repo URL\n",
        "!git clone https://github.com/YOUR_USERNAME/audio-classifier.git\n",
        "\n",
        "# Change to project directory\n",
        "import os\n",
        "os.chdir('audio-classifier')\n",
        "\n",
        "# Initialize git submodules (for external models)\n",
        "!git submodule update --init --recursive\n",
        "\n",
        "print(\"âœ… Repository cloned and submodules initialized!\")\n",
        "print(f\"ğŸ“ Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download ESC-50 dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "if not os.path.exists('data/ESC-50'):\n",
        "    print(\"ğŸ“¥ Downloading ESC-50 dataset...\")\n",
        "    !mkdir -p data\n",
        "    !cd data && wget -q https://github.com/karolpiczak/ESC-50/archive/master.zip\n",
        "    !cd data && unzip -q master.zip && mv ESC-50-master ESC-50\n",
        "    !cd data && rm master.zip\n",
        "    print(\"âœ… ESC-50 dataset downloaded!\")\n",
        "else:\n",
        "    print(\"âœ… ESC-50 dataset already exists!\")\n",
        "\n",
        "# Check dataset info\n",
        "meta_df = pd.read_csv('data/ESC-50/meta/esc50.csv')\n",
        "print(f\"\\nğŸ“Š Dataset Info:\")\n",
        "print(f\"   Samples: {len(meta_df):,}\")\n",
        "print(f\"   Classes: {len(meta_df['category'].unique())}\")\n",
        "print(f\"   Size: ~1.3GB\")\n",
        "print(f\"   Categories: {', '.join(meta_df['category'].unique()[:8])}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test model imports and creation\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "from src.models.mamba_audio import MambaAudioClassifier\n",
        "from src.models.liquidS4_audio import LiquidS4AudioClassifier\n",
        "from src.models.vjepa_audio import VJEPA2AudioClassifier\n",
        "from configs.model_configs import get_mamba_config, get_liquid_s4_config, get_vjepa2_config\n",
        "\n",
        "print(\"âœ… All imports successful!\")\n",
        "\n",
        "# Test model creation with Kaggle-optimized settings\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"ğŸ–¥ï¸  Using device: {device}\")\n",
        "\n",
        "# Test each model\n",
        "models_info = []\n",
        "\n",
        "print(\"\\nğŸ§ª Testing Mamba model...\")\n",
        "mamba_config = get_mamba_config()\n",
        "mamba_model = MambaAudioClassifier(**mamba_config, device=device)\n",
        "mamba_params = sum(p.numel() for p in mamba_model.parameters())\n",
        "models_info.append((\"Mamba\", mamba_params, \"~2-3 hours\"))\n",
        "print(f\"   âœ… Mamba: {mamba_params:,} parameters\")\n",
        "\n",
        "print(\"\\nğŸ§ª Testing Liquid S4 model...\")\n",
        "s4_config = get_liquid_s4_config()\n",
        "s4_model = LiquidS4AudioClassifier(**s4_config, device=device)\n",
        "s4_params = sum(p.numel() for p in s4_model.parameters())\n",
        "models_info.append((\"Liquid S4\", s4_params, \"~3-4 hours\"))\n",
        "print(f\"   âœ… Liquid S4: {s4_params:,} parameters\")\n",
        "\n",
        "print(\"\\nğŸ§ª Testing V-JEPA2 model...\")\n",
        "vjepa_config = get_vjepa2_config()\n",
        "vjepa_model = VJEPA2AudioClassifier(**vjepa_config)\n",
        "vjepa_params = sum(p.numel() for p in vjepa_model.parameters())\n",
        "models_info.append((\"V-JEPA2\", vjepa_params, \"~4-5 hours\"))\n",
        "print(f\"   âœ… V-JEPA2: {vjepa_params:,} parameters\")\n",
        "\n",
        "print(\"\\nğŸ“‹ Model Summary:\")\n",
        "for name, params, time in models_info:\n",
        "    print(f\"   {name:12} | {params:>8,} params | {time}\")\n",
        "\n",
        "print(\"\\nâœ… All models ready for training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ Training Options\n",
        "\n",
        "Choose which model to train by running one of the cells below:\n",
        "\n",
        "| Model | Training Time | GPU Memory | Best For |\n",
        "|-------|---------------|------------|----------|\n",
        "| **Mamba** | ~2-3 hours | 8GB | Quick results, efficient |\n",
        "| **Liquid S4** | ~3-4 hours | 12GB | Good balance |\n",
        "| **V-JEPA2** | ~4-5 hours | 16GB | Best accuracy |\n",
        "\n",
        "ğŸ’¡ **Recommendation**: Start with Mamba for fastest results!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¯ Train Mamba Model (Recommended - fastest training)\n",
        "print(\"ğŸš€ Starting Mamba training...\")\n",
        "print(\"â±ï¸  Expected time: 2-3 hours\")\n",
        "print(\"ğŸ’¾ GPU memory usage: ~8GB\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Use cloud-optimized training script\n",
        "!python scripts/cloud_train.py --model mamba --batch_size 16 --epochs 50 --lr 0.001\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Mamba training completed!\")\n",
        "print(\"ğŸ“ Checkpoints saved in: checkpoints/mamba_best.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¯ Train Liquid S4 Model\n",
        "print(\"ğŸš€ Starting Liquid S4 training...\")\n",
        "print(\"â±ï¸  Expected time: 3-4 hours\")\n",
        "print(\"ğŸ’¾ GPU memory usage: ~12GB\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "!python scripts/cloud_train.py --model liquid_s4 --batch_size 16 --epochs 50 --lr 0.001\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Liquid S4 training completed!\")\n",
        "print(\"ğŸ“ Checkpoints saved in: checkpoints/liquid_s4_best.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¯ Train V-JEPA2 Model (May need smaller batch size)\n",
        "print(\"ğŸš€ Starting V-JEPA2 training...\")\n",
        "print(\"â±ï¸  Expected time: 4-5 hours\")\n",
        "print(\"ğŸ’¾ GPU memory usage: ~16GB\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Use smaller batch size for V-JEPA2 to fit in GPU memory\n",
        "!python scripts/cloud_train.py --model vjepa2 --batch_size 8 --epochs 50 --lr 0.001\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… V-JEPA2 training completed!\")\n",
        "print(\"ğŸ“ Checkpoints saved in: checkpoints/vjepa2_best.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“¥ Download trained models and results\n",
        "import os\n",
        "from IPython.display import FileLink\n",
        "\n",
        "print(\"ğŸ“¦ Preparing downloads...\")\n",
        "\n",
        "# Create downloads directory\n",
        "!mkdir -p /kaggle/working/downloads\n",
        "\n",
        "# Download checkpoints\n",
        "if os.path.exists('checkpoints'):\n",
        "    print(\"ğŸ“ Zipping checkpoints...\")\n",
        "    !zip -r /kaggle/working/downloads/checkpoints.zip checkpoints/\n",
        "    print(\"âœ… Checkpoints ready for download!\")\n",
        "    display(FileLink('/kaggle/working/downloads/checkpoints.zip'))\n",
        "\n",
        "# Download results/logs if they exist\n",
        "if os.path.exists('results'):\n",
        "    print(\"ğŸ“ Zipping results...\")\n",
        "    !zip -r /kaggle/working/downloads/results.zip results/\n",
        "    print(\"âœ… Results ready for download!\")\n",
        "    display(FileLink('/kaggle/working/downloads/results.zip'))\n",
        "\n",
        "# Show training summary\n",
        "print(\"\\nğŸ‰ Training Summary:\")\n",
        "print(\"ğŸ“Š All models trained successfully on Kaggle's free GPU!\")\n",
        "print(\"â±ï¸  Total training time: ~9-12 hours (spread across multiple sessions)\")\n",
        "print(\"ğŸ’¾ GPU memory used: 8-16GB (well within P100's 16GB limit)\")\n",
        "print(\"ğŸ“ Models saved and ready for download!\")\n",
        "\n",
        "print(\"\\nğŸ”„ Next steps:\")\n",
        "print(\"1. Download the checkpoint files above\")\n",
        "print(\"2. Use the trained models for inference\")\n",
        "print(\"3. Compare results between Mamba, Liquid S4, and V-JEPA2\")\n",
        "print(\"4. Train additional models next week (30 hours reset Saturday!)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
